{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from langchain_huggingface import HuggingFaceEndpoint\nfrom langchain_core.prompts import PromptTemplate\n# transformers\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\nfrom transformers import pipeline\n# langchain-huggingface\nfrom langchain_huggingface.llms import HuggingFacePipeline\n\nimport torch\n\n# webscrappers\nimport requests\nimport wikipedia\n\n\nimport os\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=\"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\"\nrepo_id_mistral = \"mistralai/Mistral-7B-Instruct-v0.3\"\n\n\n#Text wrapper\nimport textwrap\ndef print_text(llm_response):\n    wrapper=textwrap.TextWrapper(width=70)\n    string=wrapper.fill(text=llm_response)\n    return print(string)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T14:54:58.678256Z","iopub.execute_input":"2024-08-20T14:54:58.679117Z","iopub.status.idle":"2024-08-20T14:54:58.686040Z","shell.execute_reply.started":"2024-08-20T14:54:58.679073Z","shell.execute_reply":"2024-08-20T14:54:58.684742Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# text data\ntopic = \"Oscillating water column\"\nwiki_article=wikipedia.WikipediaPage(topic).content\n\n## Instantiate model form huggingface\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_name=\"allenai/scibert_scivocab_uncased\"\ntokenizer=AutoTokenizer.from_pretrained(model_name)\nmodel=AutoModelForMaskedLM.from_pretrained(model_name)\n\n## Create a pipeline\npipe = pipeline(\"text-generation\", model=model,tokenizer=tokenizer,\n                max_new_tokens=128)\n## Create the llm\nllm=HuggingFacePipeline(pipeline=pipe)\n\n# Construct the template for prompt\n\n\ntemplate= \"\"\" Follow these instructions {instructions} to give the answer:\n\\n\\nContext:{context}\n\nQuestion: {query}\n\n\\nAnswer: \n\"\"\"\n\nprompt_template=PromptTemplate(input_variables=['query',\"context\", \"instructions\"],\n                      template=template)\n\n\n\n\n\n# define the chain with two components: prompt_template, llm\n\nllm_chain= prompt_template | llm\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T14:55:02.104349Z","iopub.execute_input":"2024-08-20T14:55:02.104767Z","iopub.status.idle":"2024-08-20T14:55:06.183998Z","shell.execute_reply.started":"2024-08-20T14:55:02.104731Z","shell.execute_reply":"2024-08-20T14:55:06.182816Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"The model 'BertForMaskedLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define instructions and context from wikipedia article\ninstruction=\"\"\"Instruction: Use your scientific and technical knowldege. Think like a mechanical engineer while giving the answer. If you do not \nknow the facts, do not make assumptions and avoid hallucinations and just say your knowldge is limited. Just use the context provided to you and your scientific and technical knowledge.\"\"\"\n\n## Query from user\nuser_query=\"\"\"Pakistan has a huge potential of installing OCWs at its ocean shores?\n If so which type of such plant is feasible for Pakistan considering its geological and environmental concerns?\"\"\"\n\n\n# format the template based on input variables (context and query) to make a prompt\n\nprompt = prompt_template.format(\n    instructions=instruction,\n    query=user_query,\n    context=wiki_article\n)\n\n\nmax_tokens = 512 - len(user_query) - len(instruction)\ntruncated_context = wiki_article[:max_tokens]\n# get response from the model\nresponse = llm_chain.invoke({\n    \"instructions\": instruction,\n    \"query\": user_query,\n    \"context\": truncated_context\n})\nprint_text(response)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:53:33.439956Z","iopub.execute_input":"2024-08-20T13:53:33.441397Z","iopub.status.idle":"2024-08-20T13:54:23.029748Z","shell.execute_reply.started":"2024-08-20T13:53:33.441350Z","shell.execute_reply":"2024-08-20T13:54:23.028266Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":" Follow these instructions Instruction: Use your scientific and\ntechnical knowldege. Think like a mechanical engineer while giving the\nanswer. If you do not  know the facts, do not make assumptions and\navoid hallucinations and just say your knowldge is limited. Just use\nthe context provided to you and your scientific and technical\nknowledge. to give the answer:   Context:Oscillating wat  Question:\nPakistan has a huge potential of installing OCWs at its ocean shores?\nIf so which type of such plant is feasible for Pakistan considering\nits geological and environmental concerns?   Answer:   : : : : : : : :\n: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :\n: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :\n: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :\n: : : : : : : : : : : : : : :\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llm=HuggingFaceEndpoint(repo_id=repo_id_mistral,\n                        max_new_tokens=300, \n                        temperature=0.01,\n                         model_kwargs={\"token\":\"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\",\n                                       \"add_to_git_credential\":True},\n                        )\n\n\n# build prompt template for simple question ansewring\ntemplate=\"\"\"Question: {question}\nAnswer:\"\"\"\nprompt = PromptTemplate(template=template,input_variables=['question']\n                        )\nllm_chain = prompt | llm","metadata":{},"execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n\nToken is valid (permission: write).\n\nYour token has been saved to C:\\Users\\Pakistan\\.cache\\huggingface\\token\n\nLogin successful\n"}]},{"cell_type":"code","source":"from langchain.vectorstores import Chroma\nfrom langchain_mistralai import MistralAIEmbeddings\n","metadata":{},"execution_count":null,"outputs":[]}]}