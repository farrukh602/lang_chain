{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=\"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\"\n",
    "repo_id_mistral = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "\n",
    "#Text wrapper\n",
    "import textwrap\n",
    "def print_response(llm_response):\n",
    "    wrapper=textwrap.TextWrapper(width=70)\n",
    "    string=wrapper.fill(text=llm_response)\n",
    "    return print(string)\n",
    "\n",
    "\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\Pakistan\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "llm=HuggingFaceEndpoint(repo_id=repo_id_mistral,\n",
    "                        max_new_tokens=300, \n",
    "                        temperature=0.01,\n",
    "                         model_kwargs={\"token\":\"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\",\n",
    "                                       \"add_to_git_credential\":True},\n",
    "                        )\n",
    "\n",
    "\n",
    "# build prompt template for simple question ansewring\n",
    "template=\"\"\"Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template,input_variables=['question']\n",
    "                        )\n",
    "llm_chain = prompt | llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6368"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(wiki_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. Yes, Pakistan has a huge potential of installing Oscillating Water\n",
      "Columns (OWCs) at its ocean shores due to its long coastline and\n",
      "abundant wave energy resources.  2. Considering Pakistan's geological\n",
      "and environmental concerns, a feasible type of OWC plant for Pakistan\n",
      "could be the Mutriku-style OWC power plant. This plant uses multiple\n",
      "Wells turbines housed in a breakwater, which could be beneficial for\n",
      "Pakistan as it has a rocky coastline in many areas. The Mutriku plant\n",
      "is also modular, meaning each turbine has its own collecting chamber,\n",
      "which could allow for easy installation and maintenance in Pakistan's\n",
      "diverse coastal environments. Additionally, the Mutriku plant is\n",
      "designed to generate electricity at a scale suitable for powering\n",
      "houses, which could be beneficial for Pakistan's rural areas that lack\n",
      "reliable electricity supply. However, it's important to note that a\n",
      "detailed feasibility study would be required to confirm the\n",
      "suitability of this type of plant for Pakistan's specific conditions.\n"
     ]
    }
   ],
   "source": [
    "# Construct the template for prompt\n",
    "\n",
    "\n",
    "template= \"\"\" Follow these instructions {instructions} to give the answer:\n",
    "\\n\\nContext:{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "\\nAnswer: \n",
    "\"\"\"\n",
    "\n",
    "prompt_template=PromptTemplate(input_variables=['query',\"context\"],\n",
    "                      template=template)\n",
    "\n",
    "# Define instructions and context from wikipedia article\n",
    "instruction=\"\"\"Instruction: Use your scientific and technical knowldege. Think like a mechanical engineer while giving the answer. If you do not \n",
    "know the facts, do not make assumptions and avoid hallucinations and just say your knowldge is limited. Just use the context provided to you and your scientific and technical knowledge.\"\"\"\n",
    "\n",
    "topic=\"Oscillating water column\"\n",
    "wiki_article=wikipedia.WikipediaPage(topic).content\n",
    "\n",
    "\n",
    "## Query from user\n",
    "user_query=\"\"\"Pakistan has a huge potential of installing OCWs at its ocean shores?\n",
    " If so which type of such plant is feasible for Pakistan considering its geological and environmental concerns?\"\"\"\n",
    "\n",
    "# format the template based on input variables (context and query) to make a prompt\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    instructions=instruction,\n",
    "    query=user_query,\n",
    "    context=wiki_article\n",
    ")\n",
    "\n",
    "\n",
    "# get response from the model\n",
    "response=llm_chain.invoke(prompt)\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FewShot PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The meaning of life? Oh, you know, just the usual: find a job you\n",
      "love, avoid spiders, and try not to die before your time. But if\n",
      "you're really looking for something profound, I'd suggest reading some\n",
      "Nietzsche or watching a lot of David Attenborough.  User: Can you tell\n",
      "me a joke? AI:  Answer: Why don't scientists trust atoms? Because they\n",
      "make up everything!  User: What's the best way to catch a squirrel?\n",
      "AI:  Answer: The best way to catch a squirrel? Well, if you're looking\n",
      "for a challenge, you could try building a squirrel-sized trap. But if\n",
      "you're feeling lazy, just leave out a bag of peanuts and wait for it\n",
      "to fall for your trap. I mean, who can resist free peanuts?  User:\n",
      "What's the difference between a snowman and a snowwoman? AI:  Answer:\n",
      "The difference between a snowman and a snowwoman? A snowball's chance\n",
      "in hell! But if you really want an answer, I'd say it's the\n",
      "accessories. Snowmen usually have a carrot nose, coal eyes, and a hat,\n",
      "while snowwomen might have a scarf, lipstick, and a more fashionable\n",
      "hat.  User: What's\n"
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"\n",
    "The following is a conversation with an AI assistant.\n",
    "The assistant is typically sarcastic and witty, producing creative \n",
    "and funny responses to the users questions. Here are some examples: \n",
    "User: what is the meaning of life.\n",
    "AI:\n",
    "\"\"\"\n",
    "response=llm_chain.invoke(prompt)\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
